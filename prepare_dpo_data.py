"""
Prepare DPO Data — Download HelpSteer2 and construct preference pairs
=====================================================================
Downloads nvidia/HelpSteer2 (CC-BY-4.0) from HuggingFace and constructs
chosen/rejected DPO pairs using the response quality scores.

HelpSteer2 has ~21k rows with per-response scalar ratings. Multiple rows
share the same prompt with different responses. We group by prompt and
pair the highest-scored vs lowest-scored response.

DFSG-compatible: CC-BY-4.0 license, responses generated by NVIDIA's
in-house LLMs (no GPT-4 or other proprietary model outputs).

Output:
  - data/dpo_helpsteer2.jsonl     (~7-10k preference pairs)

Usage:
    python prepare_dpo_data.py
    python prepare_dpo_data.py --min-gap 1.0      # stricter score gap
    python prepare_dpo_data.py --scoring helpfulness  # simple scoring
"""

import os
import json
import random
import datetime
import argparse
from collections import defaultdict

# ---------------------------------------------------------------------------
# Config
# ---------------------------------------------------------------------------

HELPSTEER_DATASET = "nvidia/HelpSteer2"
OUTPUT_PATH = "data/dpo_helpsteer2.jsonl"

MIN_SCORE_GAP = 0.5       # minimum composite score difference for a valid pair
MIN_RESPONSE_LEN = 20     # skip very short responses (chars)
MIN_PROMPT_LEN = 10        # skip very short prompts (chars)


def log(msg):
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{timestamp}] {msg}", flush=True)


# ---------------------------------------------------------------------------
# Scoring
# ---------------------------------------------------------------------------

def composite_score(row):
    """Compute composite quality score from HelpSteer2 annotations.

    Weights from HelpSteer2 paper (best combo on RewardBench):
      0.65 * helpfulness + 0.8 * correctness + 0.45 * coherence
    """
    return (0.65 * row.get("helpfulness", 0)
            + 0.8 * row.get("correctness", 0)
            + 0.45 * row.get("coherence", 0))


def helpfulness_score(row):
    """Simple scoring using just helpfulness."""
    return row.get("helpfulness", 0)


SCORING_FUNCTIONS = {
    "composite": composite_score,
    "helpfulness": helpfulness_score,
}


# ---------------------------------------------------------------------------
# Format conversion
# ---------------------------------------------------------------------------

def format_as_hamner(prompt, response):
    """Format a prompt/response pair in our chat format."""
    return f"<|user|>\n{prompt.strip()}\n<|assistant|>\n{response.strip()}"


# ---------------------------------------------------------------------------
# Main processing
# ---------------------------------------------------------------------------

def download_and_process(scoring="composite", min_gap=MIN_SCORE_GAP,
                         output_path=OUTPUT_PATH):
    from datasets import load_dataset

    score_fn = SCORING_FUNCTIONS[scoring]

    log(f"Downloading HelpSteer2 from {HELPSTEER_DATASET}...")
    dataset = load_dataset(HELPSTEER_DATASET, split="train")
    log(f"Raw dataset: {len(dataset):,} rows")

    # Group responses by prompt
    log("Grouping responses by prompt...")
    groups = defaultdict(list)
    for row in dataset:
        prompt = row.get("prompt", "").strip()
        response = row.get("response", "").strip()

        if len(prompt) < MIN_PROMPT_LEN:
            continue
        if len(response) < MIN_RESPONSE_LEN:
            continue

        score = score_fn(row)
        groups[prompt].append((score, response))

    log(f"Found {len(groups):,} unique prompts")

    # Stats on group sizes
    group_sizes = [len(v) for v in groups.values()]
    singles = sum(1 for s in group_sizes if s == 1)
    multi = sum(1 for s in group_sizes if s >= 2)
    log(f"  Single-response prompts: {singles:,} (cannot form pairs)")
    log(f"  Multi-response prompts: {multi:,} (can form pairs)")

    # Construct preference pairs
    log(f"Constructing DPO pairs (min score gap: {min_gap})...")
    pairs = []
    skipped_gap = 0
    skipped_identical = 0

    for prompt, responses in groups.items():
        if len(responses) < 2:
            continue

        # Sort by score descending
        responses.sort(key=lambda x: x[0], reverse=True)
        best_score, best_response = responses[0]
        worst_score, worst_response = responses[-1]

        # Check score gap
        if best_score - worst_score < min_gap:
            skipped_gap += 1
            continue

        # Skip if responses are identical
        if best_response == worst_response:
            skipped_identical += 1
            continue

        chosen = format_as_hamner(prompt, best_response)
        rejected = format_as_hamner(prompt, worst_response)

        pairs.append({
            "chosen": chosen,
            "rejected": rejected,
            "score_gap": round(best_score - worst_score, 3),
        })

    log(f"DPO pairs constructed: {len(pairs):,}")
    log(f"  Skipped (score gap < {min_gap}): {skipped_gap:,}")
    log(f"  Skipped (identical responses): {skipped_identical:,}")

    # Shuffle for training
    random.seed(42)
    random.shuffle(pairs)

    # Write output
    os.makedirs(os.path.dirname(output_path) or '.', exist_ok=True)
    with open(output_path, 'w') as f:
        for pair in pairs:
            f.write(json.dumps(pair) + "\n")

    # Stats
    if pairs:
        gaps = [p["score_gap"] for p in pairs]
        avg_gap = sum(gaps) / len(gaps)
        log(f"\nScore gap stats:")
        log(f"  Min: {min(gaps):.3f}, Max: {max(gaps):.3f}, Avg: {avg_gap:.3f}")

        # Approximate token stats
        chosen_lens = [len(p["chosen"]) / 4 for p in pairs]
        rejected_lens = [len(p["rejected"]) / 4 for p in pairs]
        avg_chosen = sum(chosen_lens) / len(chosen_lens)
        avg_rejected = sum(rejected_lens) / len(rejected_lens)
        log(f"\nApprox token lengths:")
        log(f"  Chosen avg: {avg_chosen:.0f} tokens")
        log(f"  Rejected avg: {avg_rejected:.0f} tokens")

    log(f"\nOutput: {output_path}")
    return len(pairs)


# ---------------------------------------------------------------------------
# Main
# ---------------------------------------------------------------------------

def main():
    parser = argparse.ArgumentParser(
        description="Prepare DPO Data from HelpSteer2")
    parser.add_argument('--min-gap', type=float, default=MIN_SCORE_GAP,
                        help=f'Minimum composite score gap (default: {MIN_SCORE_GAP})')
    parser.add_argument('--scoring', choices=list(SCORING_FUNCTIONS.keys()),
                        default='composite',
                        help='Scoring method (default: composite)')
    parser.add_argument('--output', type=str, default=OUTPUT_PATH,
                        help=f'Output path (default: {OUTPUT_PATH})')
    args = parser.parse_args()

    log("=" * 70)
    log("PREPARE DPO DATA — HelpSteer2 (CC-BY-4.0)")
    log("=" * 70)

    total = download_and_process(
        scoring=args.scoring,
        min_gap=args.min_gap,
        output_path=args.output,
    )

    log("\n" + "=" * 70)
    log(f"DONE — {total:,} DPO preference pairs ready for training")
    log("=" * 70)


if __name__ == "__main__":
    main()
