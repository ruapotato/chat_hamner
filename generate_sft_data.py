#!/usr/bin/env python3
"""
SFT dataset validator and merge tool for Al Hamner.

Conversations are generated by Claude Code agents into batch files under
data/personal/sft_batches/. This script validates and merges them.

Usage:
    python generate_sft_data.py --validate              # validate all batches
    python generate_sft_data.py --merge                 # merge batches into final JSONL
    python generate_sft_data.py --stats                 # show dataset statistics
    python generate_sft_data.py --progress              # show topics covered vs remaining
"""

import argparse
import json
import os
import sys
from collections import Counter
from pathlib import Path

from transformers import AutoTokenizer

# --- Config ---
TOPIC_FILES = [
    "data/personal/topics_tech.txt",
    "data/personal/topics_science.txt",
    "data/personal/topics_life.txt",
]
VOICE_SAMPLE_PATH = "data/personal/voice_sample.txt"
BATCH_DIR = "data/personal/sft_batches"
OUTPUT_PATH = "data/personal/sft_conversations.jsonl"
TOKENIZER_NAME = "HuggingFaceTB/cosmo2-tokenizer"
MAX_TOKENS = 950

SYSTEM_TAG = (
    "You are Al Hamner, a sharp-witted AI made by David Hamner. "
    "You're casual, funny, opinionated, and self-aware. "
    "You talk like a real person, not a corporate chatbot."
)


def load_topics() -> list[str]:
    topics = []
    for path in TOPIC_FILES:
        if not os.path.exists(path):
            continue
        with open(path) as f:
            for line in f:
                line = line.strip()
                if line:
                    topics.append(line)
    return topics


def validate_conversation(text: str, tokenizer) -> tuple[bool, str, int]:
    """Validate format and token count. Returns (valid, reason, token_count)."""
    if "<|system|>" not in text:
        return False, "missing <|system|>", 0
    if "<|user|>" not in text:
        return False, "missing <|user|>", 0
    if "<|assistant|>" not in text:
        return False, "missing <|assistant|>", 0

    sys_pos = text.index("<|system|>")
    user_pos = text.index("<|user|>")
    asst_pos = text.index("<|assistant|>")
    if not (sys_pos < user_pos < asst_pos):
        return False, "wrong tag order", 0

    user_count = text.count("<|user|>")
    asst_count = text.count("<|assistant|>")
    if user_count < 1 or asst_count < 1:
        return False, f"too few turns (u={user_count}, a={asst_count})", 0

    tokens = tokenizer.encode(text, add_special_tokens=False)
    n = len(tokens)
    if n > MAX_TOKENS:
        return False, f"too long ({n} tokens)", n
    if n < 40:
        return False, f"too short ({n} tokens)", n

    return True, "ok", n


def load_all_records() -> list[dict]:
    """Load all records from batch files and/or main output."""
    records = []
    # Load from batch dir
    if os.path.isdir(BATCH_DIR):
        for f in sorted(Path(BATCH_DIR).glob("*.jsonl")):
            with open(f) as fh:
                for line in fh:
                    line = line.strip()
                    if line:
                        try:
                            records.append(json.loads(line))
                        except json.JSONDecodeError:
                            pass
    return records


def cmd_validate(args):
    print("Loading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)

    records = load_all_records()
    print(f"Loaded {len(records)} records from {BATCH_DIR}/\n")

    valid_count = 0
    invalid_count = 0
    reasons = Counter()

    for i, rec in enumerate(records):
        text = rec.get("text", "")
        topic = rec.get("topic", "???")
        ok, reason, n_tok = validate_conversation(text, tokenizer)
        if ok:
            valid_count += 1
        else:
            invalid_count += 1
            reasons[reason] += 1
            if not args.quiet:
                print(f"  INVALID ({reason}): {topic[:70]}")

    print(f"\nResults: {valid_count} valid, {invalid_count} invalid out of {len(records)}")
    if reasons:
        print("Failure reasons:")
        for reason, count in reasons.most_common():
            print(f"  {count:>4d}x {reason}")


def cmd_merge(args):
    print("Loading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)

    records = load_all_records()
    print(f"Loaded {len(records)} records from batches")

    # Deduplicate by topic
    seen_topics = set()
    unique = []
    dupes = 0
    for rec in records:
        topic = rec.get("topic", "")
        if topic in seen_topics:
            dupes += 1
            continue
        seen_topics.add(topic)

        text = rec.get("text", "")
        ok, reason, n_tok = validate_conversation(text, tokenizer)
        if ok:
            unique.append(rec)

    os.makedirs(os.path.dirname(OUTPUT_PATH) or ".", exist_ok=True)
    with open(OUTPUT_PATH, "w") as f:
        for rec in unique:
            f.write(json.dumps(rec, ensure_ascii=False) + "\n")

    print(f"Merged: {len(unique)} valid unique conversations -> {OUTPUT_PATH}")
    if dupes:
        print(f"  Skipped {dupes} duplicates")


def cmd_stats(args):
    print("Loading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)

    # Check both batch dir and final output
    records = load_all_records()
    src = f"{BATCH_DIR}/"

    # Also check final output
    if os.path.exists(OUTPUT_PATH) and not records:
        with open(OUTPUT_PATH) as f:
            for line in f:
                line = line.strip()
                if line:
                    try:
                        records.append(json.loads(line))
                    except json.JSONDecodeError:
                        pass
        src = OUTPUT_PATH

    if not records:
        print("No records found.")
        return

    token_counts = []
    turn_counts = []
    for rec in records:
        text = rec.get("text", "")
        toks = tokenizer.encode(text, add_special_tokens=False)
        token_counts.append(len(toks))
        turn_counts.append(text.count("<|user|>"))

    print(f"Source: {src}")
    print(f"Conversations: {len(records)}")
    print(f"Total tokens:  {sum(token_counts):,}")
    print(f"Avg tokens:    {sum(token_counts)/len(token_counts):.0f}")
    print(f"Min tokens:    {min(token_counts)}")
    print(f"Max tokens:    {max(token_counts)}")
    print(f"Avg turns:     {sum(turn_counts)/len(turn_counts):.1f}")

    # Distribution
    buckets = [0]*10
    for n in token_counts:
        idx = min(n // 100, 9)
        buckets[idx] += 1
    print("\nToken distribution:")
    for i, count in enumerate(buckets):
        lo = i * 100
        hi = (i + 1) * 100 if i < 9 else "+"
        bar = "#" * (count * 50 // max(buckets)) if max(buckets) > 0 else ""
        print(f"  {lo:>4d}-{hi:<4} {count:>5d} {bar}")


def cmd_progress(args):
    all_topics = set(load_topics())
    print(f"Total topics: {len(all_topics)}")

    done_topics = set()
    records = load_all_records()
    for rec in records:
        done_topics.add(rec.get("topic", ""))

    # Also check final output
    if os.path.exists(OUTPUT_PATH):
        with open(OUTPUT_PATH) as f:
            for line in f:
                try:
                    done_topics.add(json.loads(line).get("topic", ""))
                except json.JSONDecodeError:
                    pass

    covered = all_topics & done_topics
    remaining = all_topics - done_topics
    pct = len(covered) / len(all_topics) * 100 if all_topics else 0

    print(f"Covered:   {len(covered)} ({pct:.1f}%)")
    print(f"Remaining: {len(remaining)}")

    if args.show_remaining:
        print("\nRemaining topics (first 50):")
        for i, t in enumerate(sorted(remaining)[:50]):
            print(f"  {t}")
        if len(remaining) > 50:
            print(f"  ... and {len(remaining) - 50} more")


def main():
    parser = argparse.ArgumentParser(description="SFT dataset validator/merge for Al Hamner")
    sub = parser.add_subparsers(dest="command")

    p_val = sub.add_parser("validate", help="Validate batch files")
    p_val.add_argument("-q", "--quiet", action="store_true")

    p_merge = sub.add_parser("merge", help="Merge batches into final JSONL")

    p_stats = sub.add_parser("stats", help="Show dataset statistics")

    p_prog = sub.add_parser("progress", help="Show generation progress")
    p_prog.add_argument("--show-remaining", action="store_true")

    # Also support --flag style for convenience
    parser.add_argument("--validate", action="store_true")
    parser.add_argument("--merge", action="store_true")
    parser.add_argument("--stats", action="store_true")
    parser.add_argument("--progress", action="store_true")
    parser.add_argument("--show-remaining", action="store_true")
    parser.add_argument("-q", "--quiet", action="store_true")

    args = parser.parse_args()

    if args.command == "validate" or args.validate:
        cmd_validate(args)
    elif args.command == "merge" or args.merge:
        cmd_merge(args)
    elif args.command == "stats" or args.stats:
        cmd_stats(args)
    elif args.command == "progress" or args.progress:
        cmd_progress(args)
    else:
        parser.print_help()


if __name__ == "__main__":
    main()
