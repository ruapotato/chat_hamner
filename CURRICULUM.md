# Hamner Curriculum Training Experiment
**Date: 2026-02-17 | Based on "Small Models, Smarter Learning" paper**

## Motivation

The original 164M param model (768h x 20L standard transformer) was trained for
122k steps (4.4B tokens) on FineWeb-Edu + 5% YouTube transcripts. It reached
loss 4.54 but couldn't produce coherent text. Scaling up to 321M+ params OOM'd
on the 24GB GPU (even at batch_size=1 for 500M+ models).

The paper "Small Models, Smarter Learning: The Power of Joint Task Training"
shows that **curriculum/joint training** can reduce minimum viable model size by
2-7x through structured embeddings. Instead of scaling up, we reorganize what
the existing model already knows.

## Approach

**Warm-start** from the existing pretrained checkpoint, then run 4-phase
curriculum training mixing structured synthetic tasks, TinyStories narratives,
FineWeb-Edu (to prevent forgetting), and personal YouTube transcripts.

### Key Ideas from the Paper
- Joint training of compatible tasks forces **structured embeddings** that
  compress knowledge more efficiently
- **Curriculum learning** (easy→hard) enables learning at 7x smaller model sizes
- **Task compatibility** (shared computational primitives) matters more than
  diversity — arithmetic + hierarchical evaluation share parsing skills that
  transfer to narrative structure

### Emotional Transformer
Middle 6 of 20 layers train at 0.2x base LR. The hypothesis: slower-learning
middle layers develop stable "emotional" representations (personality, tone,
style) while outer layers adapt quickly to input/output patterns.

## Architecture

Same model as the pretrained checkpoint — no architectural changes:

```
Model:        HamnerModel (standard transformer)
Hidden:       768
Layers:       20
Heads:        12 (4 KV heads, GQA)
MLP:          SwiGLU, intermediate=2048
Vocab:        49,152 (cosmo2 tokenizer)
Parameters:   163.6M (trainable)
Seq length:   1024
```

## Training Phases

| Phase | Steps | Synthetic | TinyStories | FineWeb | Personal | Difficulty | LR mult |
|-------|-------|-----------|-------------|---------|----------|------------|---------|
| 1. Structure | 2,000 | 60% | 20% | 15% | 5% | 1→3 | 1.0 |
| 2. Stories | 15,000 | 20% | 50% | 20% | 10% | 3→4 | 0.8 |
| 3. Voice | 10,000 | 20% | 30% | 20% | 30% | 4→5 | 0.6 |
| 4. Polish | 5,000 | 25% | 25% | 25% | 25% | 5 | 0.4 |

**Total: 32,000 steps (~1.05B tokens, ~8-9 hours)**

### Phase Details

**Phase 1: Structure** — Build computational primitives via synthetic tasks:
- Arithmetic: `"Calculate: 34 + 17 = 51"`
- Counting: `"Count the items: apple, banana, cherry. Answer: 3"`
- Bracket matching: `"Brackets: ( [ { } ] ) Valid: yes"`
- ListOps: `"[MAX 3 [MIN 7 2] 5] = 5"`
- Sorting: `"Sort: 7 2 9 1 -> 1 2 7 9"`
- Copy/Repeat: `"Repeat 3 times: hello -> hello hello hello"`

Difficulty scales 1-5 (number range, nesting depth, list length).

**Phase 2: Stories** — TinyStories (roneneldan/TinyStories on HuggingFace)
builds narrative coherence. 2.1M synthetic children's stories generated by
GPT-3.5/4. Simple vocabulary, clear narrative structure.

**Phase 3: Voice** — Increase personal data (YouTube transcripts) to 30%.
The structured embeddings from Phases 1-2 should help the model learn David's
conversational style more efficiently.

**Phase 4: Polish** — Balanced 25% each source. Consolidate all learned
representations.

## Hyperparameters

```
Base LR:          1e-4 (half of pretrain 2e-4, warm restart)
Warmup:           500 steps
LR schedule:      Cosine decay per-phase, scaled by phase lr_multiplier
Emotional LR:     Middle 6 layers at 0.2x base LR
Batch size:       32
Seq length:       1024
Optimizer:        AdamW (fresh state, betas=0.9/0.95, wd=0.1)
Mixed precision:  fp16 via GradScaler
Grad clipping:    1.0
Grad checkpointing: On
torch.compile:    On
```

## Data Sources

| Source | Description | Size |
|--------|-------------|------|
| Synthetic | On-the-fly generated structured tasks | Infinite |
| TinyStories | `roneneldan/TinyStories` (HuggingFace streaming) | 2.1M stories |
| FineWeb-Edu | `HuggingFaceFW/fineweb-edu` sample-10BT | 10B tokens |
| Personal | YouTube transcripts (`data/personal/training_samples.jsonl`) | 1,873 samples |

## Files

- `curriculum_train.py` — Main curriculum training script
- `synthetic_tasks.py` — Synthetic task generator (6 task types, difficulty 1-5)
- `logs/curriculum_metrics.csv` — Training metrics per step
- `logs/curriculum_samples.jsonl` — Sample generations per phase
- `logs/curriculum.log` — Full training log
- `checkpoints/curriculum/` — Checkpoints with phase metadata
- `plot_training.py` — Updated with `--curriculum` flag for phase-colored plots

## Running

```bash
# Start fresh from pretrained checkpoint
python curriculum_train.py

# Resume from curriculum checkpoint
python curriculum_train.py --resume

# Start from specific phase
python curriculum_train.py --phase 2

# Plot curriculum progress
python plot_training.py --curriculum --save
```

## Early Results

Phase 1 (Structure) converged within ~800 steps to loss ~1.9 / PPL ~6.6.
This confirms the paper's prediction that structured tasks converge fast.
Phase was shortened from 10k to 2k steps to avoid wasting compute on
a plateau.

## What We're Testing

1. Can curriculum training make a 164M model produce coherent text where
   4.4B tokens of raw pretraining couldn't?
2. Does the emotional transformer (slow middle layers) help capture
   personality/style?
3. What's the minimum viable training to reach coherent generation on
   this hardware?
